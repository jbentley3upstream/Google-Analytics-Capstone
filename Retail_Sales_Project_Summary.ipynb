{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b2d66-df31-4bb0-993d-dc8624b17dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreshRetailNet Multi-Region Demand Forecasting  \n",
    "### Data Science Portfolio Project – Summary Overview\n",
    "\n",
    "This project builds a complete **end-to-end Machine learning forecasting pipeline** \n",
    "for a large multi-region fresh retail dataset called FreshRetailNet-50K using\n",
    "Google Cloud Storage, BigQuery, Python, and Power BI.\n",
    "\n",
    "**Objective:**  \n",
    "Predict daily and hourly product demand at the `(store_id, product_id, date, and or hourly)` \n",
    "level to reduce waste, predict staffing needs, predict number of linehaul drivers or linehaul needs, \n",
    "and prevent stockouts, and improve inventory decisions.\n",
    "\n",
    "**Technologies:**  \n",
    "Hugging Face, Jupyter, Python, Google Cloud Storage, Google BigQuery (SQL), Vertex AI, Pandas/matplotlib/Scikit-learn, Power BI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298805c-f6a0-4222-ad7f-36c2e9e754ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Business Problem\n",
    "\n",
    "Fresh retailers struggle with overstock (waste) and understock (lost sales). \n",
    "They also struggle with accurate labor planning for both warehouses, store locations, and linehauls/trailers\n",
    "Accurate hourly forecasts help optimize:\n",
    "\n",
    "- Ordering and replenishment  \n",
    "- Labor planning  \n",
    "- Stockout prevention  \n",
    "- Reduction of perishable waste\n",
    "- Tailer planning\n",
    "\n",
    "**Key Question:**\n",
    "“How many units will each product sell in each store for each day?”\n",
    "“How many units will each product sell in each store for each hour of the day?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7932b01-cf1f-4089-8207-715cfe36045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Pipeline (High-Level)\n",
    "\n",
    "1. **Raw Data (GCS):** Parquet file containing multi-region retail sales importd into GCS from Hugging Face.  \n",
    "2. **BigQuery:**\n",
    "   - Load raw parquet into staging table  \n",
    "   - Create a deduplicated, partitioned (by date) and clustered (store, product) table  \n",
    "   - Join with holiday table to include the holiday feature as a possibly useful feature in the future.  \n",
    "   - Produce final ML table: `FreshRetailData_for_machine_learning_multi_region`\n",
    "   - Produce sampled version of final ML table that will be used for analysis and model building: `fresh_retail_sampled`\n",
    "3. **Jupyter Modeling:**\n",
    "   - Load data via BigQuery client\n",
    "   - Remove outliers using advanced triple exponential smoothing algorithms\n",
    "   - Clean and engineer features using triple exponential smoothing algorithms, regression, and moving average\n",
    "   - Prepare and transform data into a structured format optimized for machine learning modeling \n",
    "   - Train and compare forecasting models\n",
    "4. **Hour of day curve**\n",
    "    - the data exploration, feature engineering, and model development is conducted at the daily level\n",
    "    - Therefore an hourly level of the forecast needs to be created\n",
    "    - Develop a hour of day curve based by dividing the hourly volume by the summed hourly volume for each day.\n",
    "    - Do this for the most recent 3 weeks of actuals and then calculate a weighted average of the past 3 weeks\n",
    "    - Apply the hourly curve to the daily forecast to get the more granular hourly forecast.\n",
    "5. **Power BI Dashboard:**\n",
    "   - Visualize forecasts vs. actuals at both the daily and hourly level  \n",
    "   - Provide store/product filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d18e64-8615-4dea-8712-f5c45109a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Dataset & Feature Engineering\n",
    "\n",
    "**Key fields used:**\n",
    "- `store_id`, `product_id`, `date_feature`, `hour`, `hours_sale`\n",
    "\n",
    "**Other important fields:**\n",
    "- Time features: week_sat_fri, week_day_name, date_feature, hour, Weekend Classifier  \n",
    "- Lag features: 1-day and 7-day lags  \n",
    "- Rolling means: 7-day and 14-day SMAs     \n",
    "# The lagged forecasted features created by the triple exponential smoothing algorithm \n",
    "#are used to provide additional information and patterns for the machine learning model.\n",
    "\n",
    "These features capture **trend**, **seasonality**, **level**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dc865-26aa-429d-9dbb-e5c2fbd03637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Modeling Summary\n",
    "\n",
    "### Classical Time-Series Analysis\n",
    "- Triple Exponential Smoothing (SES) was used for outlier removal and feature engineering.\n",
    "- Many features were engineered from the SES in this analysis\n",
    "\n",
    "#Several features were built and compared:\n",
    "- Linear regression and moving average algorithms were run at the \n",
    "product by store id level to create features\n",
    "\n",
    "# Engineered features\n",
    "       'week_day_name', 'Forecast', 'Level',\n",
    "       'Trend', 'Season', 'date_rank', 'SMA_7' (moving average), 'Regression',\n",
    "       'Average Seasonality', 'Forecast_7day_lag', 'Level_7day_lag',\n",
    "       'Trend_7day_lag', 'Season_7day_lag', 'Regression_7day_lag',\n",
    "       'SMA_7_7day_lag', 'Weekend Classifier'\n",
    "\n",
    "\n",
    "### Machine Learning\n",
    "Multiple machine learning models were explored and tested by the data scientist\n",
    "such as Decision Tree, Random Forest, Gradient Boosting and more\n",
    "during the develpment of this model, but only one \n",
    "was selected to be used for this production model and is available for view \n",
    "within the jupyter notebook of the production model, the Neural Network.\n",
    "\n",
    "- Neural Network (MLPRegressor with tuned parameters)\n",
    "\n",
    "### Evaluation\n",
    "Train/test split respecting time order.  \n",
    "Metrics used: **RMSE**, **MAE**, BIAS.\n",
    "\n",
    "**Top two performing models:**\n",
    "- **Neural Network (MLP)** (only one shown) \n",
    "- **Gradient Boosting (not included in this production model)\n",
    "\n",
    "Both consistently outperformed baselines and classical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd6014-9860-4d6c-8c4d-5880ba872067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Key Results\n",
    "## store by product selection: store_id = 7 and product_id = 4\n",
    "# This combination is selected as an example here due to its past data showing clear \n",
    "#patterns of seasonality, Trend, as well as changes in level\n",
    "# More details are available in the notebook called \n",
    "# Retail_Sales_Production_Model\n",
    "| Model                  | RMSE (Test) | MAE (Test) |\n",
    "|-----------------------|-------------|-----------| \n",
    "| Neural Network (MLP)  | 25%        | 17%    |\n",
    "\n",
    "**Interpretation:**  \n",
    "Tree-based and neural methods capture nonlinear relationships between \n",
    "the engineered features than classical time-series models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea69991-123e-4646-abf0-99b76ab0fa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbca69c-bb33-4758-b083-0f6d56a57bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a568cf-c6bc-4af6-ba49-804b1921ab2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27af97-314e-4ca4-b714-c98f584e4733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
